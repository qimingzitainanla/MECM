{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99afac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.utils.data as Data\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import einsum\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, LeaveOneGroupOut\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from tqdm import tqdm, trange\n",
    "from torcheeg.datasets.constants.emotion_recognition import format_region_channel_list\n",
    "import random\n",
    "import mne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84db2e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1698, 1, 62, 875]),\n",
       " torch.Size([1698]),\n",
       " torch.Size([1698]),\n",
       " torch.Size([425, 1, 62, 875]),\n",
       " torch.Size([425]),\n",
       " torch.Size([425]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=torch.from_numpy(np.load(f'data_training/eright_and_cwrong_train_data.npy'))\n",
    "train_dec_label=torch.from_numpy(np.load(f'data_training/eright_and_cwrong_train_dec_label.npy'))\n",
    "train_emo_label=torch.from_numpy(np.load(f'data_training/eright_and_cwrong_train_emo_label.npy'))\n",
    "test_data=torch.from_numpy(np.load(f'data_training/eright_and_cwrong_test_data.npy'))\n",
    "test_dec_label=torch.from_numpy(np.load(f'data_training/eright_and_cwrong_test_dec_label.npy'))\n",
    "test_emo_label=torch.from_numpy(np.load(f'data_training/eright_and_cwrong_test_emo_label.npy'))\n",
    "train_data.shape,train_dec_label.shape,train_emo_label.shape,test_data.shape,test_dec_label.shape,test_emo_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3fdbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">\n",
       "    // must be `var` (not `const`) because this can get embedded multiple times on a page\n",
       "var toggleVisibility = (className) => {\n",
       "\n",
       "    const elements = document.querySelectorAll(`.${className}`);\n",
       "\n",
       "    elements.forEach(element => {\n",
       "        if (element.classList.contains(\"mne-repr-section-header\")) {\n",
       "            return  // Don't collapse the section header row\n",
       "        }\n",
       "        element.classList.toggle(\"mne-repr-collapsed\");\n",
       "    });\n",
       "\n",
       "    // trigger caret to rotate\n",
       "    var sel = `.mne-repr-section-header.${className} > th.mne-repr-section-toggle > button`;\n",
       "    const button = document.querySelector(sel);\n",
       "    button.classList.toggle(\"collapsed\");\n",
       "\n",
       "    // adjust tooltip\n",
       "    sel = `tr.mne-repr-section-header.${className}`;\n",
       "    const secHeadRow = document.querySelector(sel);\n",
       "    secHeadRow.classList.toggle(\"collapsed\");\n",
       "    secHeadRow.title = secHeadRow.title === \"Hide section\" ? \"Show section\" : \"Hide section\";\n",
       "}\n",
       "</script>\n",
       "\n",
       "<style type=\"text/css\">\n",
       "    /*\n",
       "Styles in this section apply both to the sphinx-built website docs and to notebooks\n",
       "rendered in an IDE or in Jupyter. In our web docs, styles here are complemented by\n",
       "doc/_static/styles.css and other CSS files (e.g. from the sphinx theme, sphinx-gallery,\n",
       "or bootstrap). In IDEs/Jupyter, those style files are unavailable, so only the rules in\n",
       "this file apply (plus whatever default styling the IDE applies).\n",
       "*/\n",
       ".mne-repr-table {\n",
       "    display: inline;  /* prevent using full container width */\n",
       "}\n",
       ".mne-repr-table tr.mne-repr-section-header > th {\n",
       "    padding-top: 1rem;\n",
       "    text-align: left;\n",
       "    vertical-align: middle;\n",
       "}\n",
       ".mne-repr-section-toggle > button {\n",
       "    all: unset;\n",
       "    display: block;\n",
       "    height: 1rem;\n",
       "    width: 1rem;\n",
       "}\n",
       ".mne-repr-section-toggle > button > svg {\n",
       "    height: 60%;\n",
       "}\n",
       "\n",
       "/* transition (rotation) effects on the collapser button */\n",
       ".mne-repr-section-toggle > button.collapsed > svg {\n",
       "    transition: 0.1s ease-out;\n",
       "    transform: rotate(-90deg);\n",
       "}\n",
       ".mne-repr-section-toggle > button:not(.collapsed) > svg {\n",
       "    transition: 0.1s ease-out;\n",
       "    transform: rotate(0deg);\n",
       "}\n",
       "\n",
       "/* hide collapsed table rows */\n",
       ".mne-repr-collapsed {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "\n",
       "@layer {\n",
       "    /*\n",
       "    Selectors in a `@layer` will always be lower-precedence than selectors outside the\n",
       "    layer. So even though e.g. `div.output_html` is present in the sphinx-rendered\n",
       "    website docs, the styles here won't take effect there as long as some other rule\n",
       "    somewhere in the page's CSS targets the same element.\n",
       "\n",
       "    In IDEs or Jupyter notebooks, though, the CSS files from the sphinx theme,\n",
       "    sphinx-gallery, and bootstrap are unavailable, so these styles will apply.\n",
       "\n",
       "    Notes:\n",
       "\n",
       "    - the selector `.accordion-body` is for MNE Reports\n",
       "    - the selector `.output_html` is for VSCode's notebook interface\n",
       "    - the selector `.jp-RenderedHTML` is for Jupyter notebook\n",
       "    - variables starting with `--theme-` are VSCode-specific.\n",
       "    - variables starting with `--jp-` are Jupyter styles, *some of which* are also\n",
       "      available in VSCode. Here we try the `--theme-` variable first, then fall back to\n",
       "      the `--jp-` ones.\n",
       "    */\n",
       "    .mne-repr-table {\n",
       "        --mne-toggle-color: var(--theme-foreground, var(--jp-ui-font-color1));\n",
       "        --mne-button-bg-color: var(--theme-button-background, var(--jp-info-color0, var(--jp-content-link-color)));\n",
       "        --mne-button-fg-color: var(--theme-button-foreground, var(--jp-ui-inverse-font-color0, var(--jp-editor-background)));\n",
       "        --mne-button-hover-bg-color: var(--theme-button-hover-background, var(--jp-info-color1));\n",
       "        --mne-button-radius: var(--jp-border-radius, 0.25rem);\n",
       "    }\n",
       "    /* chevron position/alignment; in VSCode it looks ok without adjusting */\n",
       "    .accordion-body .mne-repr-section-toggle > button,\n",
       "    .jp-RenderedHTML .mne-repr-section-toggle > button {\n",
       "        padding: 0 0 45% 25% !important;\n",
       "    }\n",
       "    /* chevron color; MNE Report doesn't have light/dark mode */\n",
       "    div.output_html .mne-repr-section-toggle > button > svg > path,\n",
       "    .jp-RenderedHTML .mne-repr-section-toggle > button > svg > path {\n",
       "        fill: var(--mne-toggle-color);\n",
       "    }\n",
       "    .accordion-body .mne-ch-names-btn,\n",
       "    div.output_html .mne-ch-names-btn,\n",
       "    .jp-RenderedHTML .mne-ch-names-btn {\n",
       "        -webkit-border-radius: var(--mne-button-radius);\n",
       "        -moz-border-radius: var(--mne-button-radius);\n",
       "        border-radius: var(--mne-button-radius);\n",
       "        border: none;\n",
       "        background-image: none;\n",
       "        background-color: var(--mne-button-bg-color);\n",
       "        color: var(--mne-button-fg-color);\n",
       "        font-size: inherit;\n",
       "        min-width: 1.5rem;\n",
       "        padding: 0.25rem;\n",
       "        text-align: center;\n",
       "        text-decoration: none;\n",
       "    }\n",
       "    .accordion-body .mne-ch-names-btn:hover,\n",
       "    div.output_html .mne.ch-names-btn:hover,\n",
       "    .jp-RenderedHTML .mne-ch-names-btn:hover {\n",
       "        background-color: var(--mne-button-hover-bg-color);\n",
       "        text-decoration: underline;\n",
       "    }\n",
       "    .accordion-body .mne-ch-names-btn:focus-visible,\n",
       "    div.output_html .mne-ch-names-btn:focus-visible,\n",
       "    .jp-RenderedHTML .mne-ch-names-btn:focus-visible {\n",
       "        outline: 0.1875rem solid var(--mne-button-bg-color) !important;\n",
       "        outline-offset: 0.1875rem !important;\n",
       "    }\n",
       "}\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "<table class=\"table mne-repr-table\">\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>General</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Filename(s)</td>\n",
       "    <td>\n",
       "        \n",
       "        preprocessed_01102201.set\n",
       "        \n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "<tr class=\"repr-element general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>MNE object type</td>\n",
       "    <td>RawEEGLAB</td>\n",
       "</tr>\n",
       "<tr class=\"repr-element general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Measurement date</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Participant</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "<tr class=\"repr-element general-a08bdb55-7dcc-43c2-a825-e92cca1e5c7e \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Experimenter</td>\n",
       "    \n",
       "    <td>Unknown</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header acquisition-73e0ef1f-3b19-468d-a4cd-9431c873dc1c\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('acquisition-73e0ef1f-3b19-468d-a4cd-9431c873dc1c')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Acquisition</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-73e0ef1f-3b19-468d-a4cd-9431c873dc1c \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Duration</td>\n",
       "    <td>00:11:00 (HH:MM:SS)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-73e0ef1f-3b19-468d-a4cd-9431c873dc1c \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Sampling frequency</td>\n",
       "    <td>250.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element acquisition-73e0ef1f-3b19-468d-a4cd-9431c873dc1c \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Time points</td>\n",
       "    <td>164,825</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header channels-02c3bc3b-a063-4972-8d2d-2f0d401e28e6\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('channels-02c3bc3b-a063-4972-8d2d-2f0d401e28e6')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Channels</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "    \n",
       "<tr class=\"repr-element channels-02c3bc3b-a063-4972-8d2d-2f0d401e28e6 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>EEG</td>\n",
       "    <td>\n",
       "        <button class=\"mne-ch-names-btn sd-sphinx-override sd-btn sd-btn-info sd-text-wrap sd-shadow-sm\" onclick=\"alert('Good EEG:\\n\\nFP1, FPZ, FP2, AF3, AF4, F7, F5, F3, F1, FZ, F2, F4, F6, F8, FT7, FC5, FC3, FC1, FCZ, FC2, FC4, FC6, FT8, T7, C5, C3, C1, CZ, C2, C4, C6, T8, TP7, CP5, CP3, CP1, CPZ, CP2, CP4, CP6, TP8, P7, P5, P3, P1, PZ, P2, P4, P6, P8, PO7, PO5, PO3, POZ, PO4, PO6, PO8, CB1, O1, OZ, O2, CB2')\" title=\"(Click to open in popup)&#13;&#13;FP1, FPZ, FP2, AF3, AF4, F7, F5, F3, F1, FZ, F2, F4, F6, F8, FT7, FC5, FC3, FC1, FCZ, FC2, FC4, FC6, FT8, T7, C5, C3, C1, CZ, C2, C4, C6, T8, TP7, CP5, CP3, CP1, CPZ, CP2, CP4, CP6, TP8, P7, P5, P3, P1, PZ, P2, P4, P6, P8, PO7, PO5, PO3, POZ, PO4, PO6, PO8, CB1, O1, OZ, O2, CB2\">\n",
       "            62\n",
       "        </button>\n",
       "\n",
       "        \n",
       "    </td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element channels-02c3bc3b-a063-4972-8d2d-2f0d401e28e6 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Head & sensor digitization</td>\n",
       "    \n",
       "    <td>67 points</td>\n",
       "    \n",
       "</tr>\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<tr class=\"mne-repr-section-header filters-00514837-7362-422c-95a4-473c1136e8d5\"\n",
       "     title=\"Hide section\" \n",
       "    onclick=\"toggleVisibility('filters-00514837-7362-422c-95a4-473c1136e8d5')\">\n",
       "    <th class=\"mne-repr-section-toggle\">\n",
       "        <button >\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 512 512\"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d=\"M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z\"/></svg>\n",
       "        </button>\n",
       "    </th>\n",
       "    <th colspan=\"2\">\n",
       "        <strong>Filters</strong>\n",
       "    </th>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-00514837-7362-422c-95a4-473c1136e8d5 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Highpass</td>\n",
       "    <td>0.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "<tr class=\"repr-element filters-00514837-7362-422c-95a4-473c1136e8d5 \">\n",
       "    <td class=\"mne-repr-section-toggle\"></td>\n",
       "    <td>Lowpass</td>\n",
       "    <td>125.00 Hz</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>"
      ],
      "text/plain": [
       "<RawEEGLAB | preprocessed_01102201.set, 62 x 164825 (659.3 s), ~78.1 MiB, data loaded>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data_now = mne.io.read_raw_eeglab('F:/preprocessed_01102201.set', preload=True)\n",
    "# 要删除的通道名字列表\n",
    "channels_to_remove = ['M1', 'M2','HEO','VEO','Trigger']\n",
    "\n",
    "# 使用 pick_channels 删除指定的通道\n",
    "eeg_data_now.pick_channels(ch_names=[ch for ch in eeg_data_now.ch_names if ch not in channels_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7dea47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION_LIST = [\n",
    "    [ '-', '-', '-', 'FP1', 'FPZ', 'FP2', '-', '-', '-'],\n",
    "    [ '-', '-', '-', 'AF3', '-', 'AF4', '-', '-', '-'],\n",
    "    [ 'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8'],\n",
    "    [ 'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8'],\n",
    "    [ 'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8'],\n",
    "    [ 'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8'],\n",
    "    [ 'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8'],\n",
    "    [ '-', 'PO7', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'PO8', '-'],\n",
    "    [ '-', '-', 'CB1', 'O1', 'OZ', 'O2', 'CB2', '-', '-' ]\n",
    "]\n",
    "HEMISPHERE_LIST = [\n",
    "    ['F7', 'F5', 'F3', 'F1'], \n",
    "    ['F2', 'F4', 'F6', 'F8'],\n",
    "    [ 'FC5', 'FC3', 'FC1'],\n",
    "    ['FC2', 'FC4', 'FC6'], \n",
    "    ['FP1',  'AF3'], \n",
    "    ['FP2', 'AF4', ],\n",
    "    ['FPZ', 'FZ', 'FCZ', 'CZ', 'CPZ', 'PZ', 'POZ', 'OZ'],\n",
    "    ['C5', 'C3', 'C1'], ['C2', 'C4', 'C6'], ['CP5', 'CP3', 'CP1'],\n",
    "    ['CP2', 'CP4', 'CP6'], \n",
    "    ['P7', 'P5', 'P3', 'P1'], \n",
    "    ['P2', 'P4', 'P6', 'P8'],\n",
    "    ['PO7','PO5', 'PO3', 'O1', 'CB1'], \n",
    "    ['PO4', 'PO6', 'PO8', 'O2', 'CB2'], \n",
    "    ['FT7','T7', 'TP7'],\n",
    "    ['FT8','T8', 'TP8' ]\n",
    "]\n",
    "CHANNELS_LIST = eeg_data_now.ch_names\n",
    "\n",
    "GENERAL_REGION_LIST = format_region_channel_list(CHANNELS_LIST, LOCATION_LIST)     #按行分\n",
    "\n",
    "HEMISPHERE_REGION_LIST = format_region_channel_list(CHANNELS_LIST, HEMISPHERE_LIST)            #按区域分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2acdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channelsnel, ratio=2):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channelsnel,\n",
    "                             in_channelsnel // ratio,\n",
    "                             1,\n",
    "                             bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_channelsnel // ratio,\n",
    "                             in_channelsnel,\n",
    "                             1,\n",
    "                             bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool_out = self.avg_pool(x)\n",
    "        max_pool_out = self.max_pool(x)\n",
    "        avg_pool_out = self.fc2(self.relu1(self.fc1(avg_pool_out)))\n",
    "        max_pool_out = self.fc2(self.relu1(self.fc1(max_pool_out)))\n",
    "        out = max_pool_out + avg_pool_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'Kernel size must be 3 or 7.'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        avg_pool_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_pool_out, max_pool_out], dim=1)\n",
    "        out = self.conv1(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_channelsnel, ratio=2, kernel_size=7):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.cha_att = ChannelAttention(in_channelsnel, ratio=ratio)\n",
    "        self.spa_att = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x * self.cha_att(x)\n",
    "        out = out * self.spa_att(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PowerLayer(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(PowerLayer, self).__init__()\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=(1, kernel_size),\n",
    "                                    stride=(1, stride))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(self.pooling(x.pow(2)))\n",
    "\n",
    "\n",
    "class Aggregator():\n",
    "    def __init__(self, region_list):\n",
    "        self.region_list = region_list\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for region_index in range(len(self.region_list)):\n",
    "            region_x = x[:, self.region_list[region_index], :]\n",
    "            aggr_region_x = torch.mean(region_x, dim=1)\n",
    "            output.append(aggr_region_x)\n",
    "        return torch.stack(output, dim=1)\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = Parameter(torch.FloatTensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(\n",
    "                torch.zeros((1, 1, out_channels), dtype=torch.float32))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        nn.init.xavier_uniform_(self.weight, gain=1.414)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        output = torch.matmul(x, self.weight) - self.bias\n",
    "        output = F.relu(torch.matmul(adj, output))\n",
    "        return output\n",
    "\n",
    "\n",
    "class MECM_EDNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 region_list,\n",
    "                 in_channels: int = 1,\n",
    "                 num_electrodes: int = 32,\n",
    "                 chunk_size: int = 128,\n",
    "                 sampling_rate: int = 128,\n",
    "                 num_T: int = 64,\n",
    "                 hid_channels: int = 32,\n",
    "                 dropout: float = 0.5,\n",
    "                 pool_kernel_size: int = 16,\n",
    "                 pool_stride: int = 4,\n",
    "                 num_classes_dec: int = 2,\n",
    "                num_classes_emo:int = 4):\n",
    "        super(MECM_EDNet, self).__init__()\n",
    "        self.region_list = region_list\n",
    "        self.inception_window = [0.5, 0.25, 0.125]\n",
    "\n",
    "        self.num_classes_dec = num_classes_dec\n",
    "        self.num_classes_emo= num_classes_emo\n",
    "        self.in_channels = in_channels\n",
    "        self.num_electrodes = num_electrodes\n",
    "        self.chunk_size = chunk_size\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.num_T = num_T\n",
    "        self.hid_channels = hid_channels\n",
    "        self.dropout = dropout\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pool_stride = pool_stride\n",
    "        self.in_channels = in_channels\n",
    "        self.num_electrodes = num_electrodes\n",
    "\n",
    "        self.t_block1 = self.temporal_block(\n",
    "            self.in_channels, self.num_T,\n",
    "            (1, int(self.inception_window[0] * self.sampling_rate)),\n",
    "            self.pool_kernel_size, self.pool_stride)\n",
    "        self.t_block2 = self.temporal_block(\n",
    "            self.in_channels, self.num_T,\n",
    "            (1, int(self.inception_window[1] * self.sampling_rate)),\n",
    "            self.pool_kernel_size, self.pool_stride)\n",
    "        self.t_block3 = self.temporal_block(\n",
    "            self.in_channels, self.num_T,\n",
    "            (1, int(self.inception_window[2] * self.sampling_rate)),\n",
    "            self.pool_kernel_size, self.pool_stride)\n",
    "\n",
    "        self.bn_t1 = nn.BatchNorm2d(self.num_T)\n",
    "        self.bn_t2 = nn.BatchNorm2d(self.num_T)\n",
    "\n",
    "        self.cbam = CBAMBlock(num_electrodes)\n",
    "\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv2d(num_T, num_T, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            nn.LeakyReLU(), nn.AvgPool2d((1, 2)))\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d((1, 2))\n",
    "\n",
    "        feature_dim = self.feature_dim\n",
    "        self.local_filter_weight = nn.Parameter(torch.FloatTensor(\n",
    "            self.num_electrodes, feature_dim),\n",
    "                                                requires_grad=True)\n",
    "        self.local_filter_bias = nn.Parameter(torch.zeros(\n",
    "            (1, self.num_electrodes, 1), dtype=torch.float32),\n",
    "                                              requires_grad=True)\n",
    "\n",
    "        self.aggregate = Aggregator(self.region_list)\n",
    "        num_region = len(self.region_list)\n",
    "\n",
    "        self.global_adj = nn.Parameter(torch.FloatTensor(\n",
    "            num_region, num_region),\n",
    "                                       requires_grad=True)\n",
    "\n",
    "        self.bn_g1 = nn.BatchNorm1d(num_region)\n",
    "        self.bn_g2 = nn.BatchNorm1d(num_region)\n",
    "\n",
    "        self.gcn = GraphConvolution(feature_dim, hid_channels)\n",
    "\n",
    "        self.fc_dec = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(int(num_region * hid_channels), num_classes_dec))\n",
    "        \n",
    "        self.fc_emo = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(int(num_region * hid_channels), num_classes_emo))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.local_filter_weight)\n",
    "        nn.init.xavier_uniform_(self.global_adj)\n",
    "\n",
    "    def temporal_block(self, in_channels, out_channels, kernel_size,\n",
    "                       pool_kernel_size, pool_stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=(1, 1)),\n",
    "            PowerLayer(kernel_size=pool_kernel_size, stride=pool_stride))\n",
    "\n",
    "    def forward(self, x):\n",
    "        r'''\n",
    "        Args:\n",
    "            x (torch.Tensor): EEG signal representation, the ideal input shape is :obj:`[n, 1, 32, 128]`. Here, :obj:`n` corresponds to the batch size, :obj:`32` corresponds to :obj:`num_electrodes`, and :obj:`chunk_size` corresponds to :obj:`chunk_size`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor[number of sample, number of classes]: the predicted probability that the samples belong to the classes.\n",
    "        '''\n",
    "        t1 = self.t_block1(x)\n",
    "        t2 = self.t_block2(x)\n",
    "        t3 = self.t_block3(x)\n",
    "        x = torch.cat((t1, t2, t3), dim=-1)\n",
    "\n",
    "        x = self.bn_t1(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = self.cbam(x)\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        x = x.flatten(start_dim=2)\n",
    "        x = self.local_filter(x)\n",
    "        x = self.aggregate.forward(x)\n",
    "        adj = self.get_adj(x)\n",
    "        x = self.bn_g1(x)\n",
    "\n",
    "        x = self.gcn(x, adj)\n",
    "        x = self.bn_g2(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x1 = self.fc_dec(x)\n",
    "        x2= self.fc_emo(x)\n",
    "        return x1,x2,x\n",
    "\n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        mock_eeg = torch.randn(\n",
    "            (1, self.in_channels, self.num_electrodes, self.chunk_size))\n",
    "\n",
    "        t1 = self.t_block1(mock_eeg)\n",
    "        t2 = self.t_block2(mock_eeg)\n",
    "        t3 = self.t_block3(mock_eeg)\n",
    "        mock_eeg = torch.cat((t1, t2, t3), dim=-1)\n",
    "\n",
    "        mock_eeg = self.bn_t1(mock_eeg)\n",
    "        mock_eeg = self.conv1x1(mock_eeg)\n",
    "        mock_eeg = self.bn_t2(mock_eeg)\n",
    "        mock_eeg = mock_eeg.permute(0, 2, 1, 3)\n",
    "        mock_eeg = mock_eeg.flatten(start_dim=2)\n",
    "        return mock_eeg.shape[-1]\n",
    "\n",
    "    def local_filter(self, x):\n",
    "        w = self.local_filter_weight.unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "        x = F.relu(torch.mul(x, w) - self.local_filter_bias)\n",
    "        return x\n",
    "\n",
    "    def get_adj(self, x, self_loop=True):\n",
    "        adj = torch.bmm(x, x.permute(0, 2, 1))\n",
    "        num_nodes = adj.shape[-1]\n",
    "        adj = F.relu(adj * (self.global_adj + self.global_adj.transpose(1, 0)))\n",
    "        if self_loop:\n",
    "            adj = adj + torch.eye(num_nodes).to(x.device)\n",
    "        rowsum = torch.sum(adj, dim=-1)\n",
    "        mask = torch.zeros_like(rowsum)\n",
    "        mask[rowsum == 0] = 1\n",
    "        rowsum += mask\n",
    "        d_inv_sqrt = torch.pow(rowsum, -0.5)\n",
    "        d_mat_inv_sqrt = torch.diag_embed(d_inv_sqrt)\n",
    "        adj = torch.bmm(torch.bmm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da189cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\python310\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import os, shutil\n",
    "# from ResNet_model import ResNet\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "import argparse\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "device = torch.device('cuda')\n",
    "import torch.nn.functional as F\n",
    "#减数分裂代码\n",
    "def meiosis(signal):\n",
    "    num_pieces=int(len(signal)/2)\n",
    "    rand_subs_stre = random.sample(range(0, num_pieces*2), num_pieces*2)\n",
    "    split=random.randint(1,signal.shape[-1]-2)\n",
    "    new_signal = []\n",
    "    new_signal1 = []\n",
    "    new_signal2 = []\n",
    "    for i in range(num_pieces):\n",
    "        si = rand_subs_stre[i]\n",
    "        sj = rand_subs_stre[i+num_pieces]\n",
    "#         print(signal.shape)\n",
    "        xi = np.concatenate([signal[si,:,:,:split],signal[sj,:,:,split:]],axis=2)\n",
    "#         print(xi.shape)\n",
    "        xj = np.concatenate([signal[sj,:,:,:split],signal[si,:,:,split:]],axis=2)\n",
    "        new_signal1.append(xi)\n",
    "        new_signal2.append(xj)\n",
    "    new_signal = new_signal1 + new_signal2 #合并list\n",
    "    new_signal = np.array(new_signal)\n",
    "    return new_signal,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa482b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        # L2 normalize the features\n",
    "        labels=labels.reshape(-1,1)\n",
    "        features = F.normalize(features, dim=-1, p=2)\n",
    "\n",
    "        # Compute the cosine similarity between features\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "\n",
    "        # Build a mask to separate positive and negative pairs\n",
    "        mask = torch.eq(labels, labels.mT).float()\n",
    "        # Calculate the log probability of positive pairs\n",
    "        log_prob = similarity_matrix - (mask * 1e9)\n",
    "\n",
    "        # Compute the mean loss for positive pairs\n",
    "        loss = -torch.log(torch.exp(log_prob).sum(1) / torch.exp(log_prob).sum(1).sum())\n",
    "\n",
    "#         if np.isnan(loss).any():\n",
    "#             print(torch.exp(log_prob).sum(1),log_prob)\n",
    "            \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57b9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def MECM_EDNet_learning(alpha,beta,gamma):\n",
    "\n",
    "\n",
    "    # for i in range(len(model_name_list)):\n",
    "    #     model_name=model_name_list[i]\n",
    "#######################基本参数########################################\n",
    "    device = torch.device(\"cuda:0\")\n",
    "#     print(class_weights)\n",
    "    criterion1 = nn.CrossEntropyLoss()\n",
    "    criterion2 = nn.CrossEntropyLoss()\n",
    "    criterion3 = SupervisedContrastiveLoss()\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    epoch_size = 13\n",
    "\n",
    "    save = True\n",
    "\n",
    "    model_name = 'MeCM_EDNet'\n",
    "###########################################################################\n",
    "    # 创建正样本和负样本的索引列表\n",
    "    positive_indices = (train_dec_label == 0).nonzero().squeeze()\n",
    "    negative_indices = (train_dec_label == 1).nonzero().squeeze()\n",
    "\n",
    "    print(positive_indices.shape,negative_indices.shape)\n",
    "    # 定义批次大小\n",
    "    batch_size1 = 17\n",
    "    batch_size2 = 15\n",
    "\n",
    "    # 创建正样本和负样本的数据增强的训练数据集\n",
    "    # positive_dataset = TensorDataset(train_data[positive_indices], train_emo_label[positive_indices])\n",
    "    # negative_dataset = TensorDataset(train_data[negative_indices], train_emo_label[negative_indices])\n",
    "    positive_data=train_data[positive_indices]\n",
    "    negative_data=train_data[negative_indices]\n",
    "\n",
    "    enhanced_positive_data,spilt=meiosis(positive_data)\n",
    "    enhanced_negative_data,spilt=meiosis(negative_data)\n",
    "\n",
    "    enhanced_positive_data=torch.tensor(enhanced_positive_data)\n",
    "    enhanced_negative_data=torch.tensor(enhanced_negative_data)\n",
    "    train_pos_data=torch.cat((positive_data,enhanced_positive_data),axis=0)\n",
    "    train_neg_data=torch.cat((negative_data,enhanced_negative_data),axis=0)\n",
    "\n",
    "    #把emo标签拼接完成\n",
    "    length1=int(np.floor(len(positive_indices)/2)*2+len(positive_indices))\n",
    "    length2=int(np.floor(len(negative_indices)/2)*2+len(negative_indices))\n",
    "    pos_emo_label=torch.cat((train_emo_label[positive_indices],train_emo_label[positive_indices]),axis=0)[:length1]\n",
    "    neg_emo_label=torch.cat((train_emo_label[negative_indices],train_emo_label[negative_indices]),axis=0)[:length2]\n",
    "\n",
    "    positive_dataset = TensorDataset(train_pos_data,pos_emo_label)\n",
    "    negative_dataset = TensorDataset(train_neg_data,neg_emo_label)\n",
    "\n",
    "    # 创建两个训练数据加载器，一个用于正样本，一个用于负样本\n",
    "    positive_dataloader = DataLoader(positive_dataset, batch_size=batch_size1, shuffle=True)\n",
    "    negative_dataloader = DataLoader(negative_dataset, batch_size=batch_size2, shuffle=True)\n",
    "\n",
    "    #测试数据加载\n",
    "    test_set = TensorDataset(test_data, test_dec_label,test_emo_label)\n",
    "    test_loader = Data.DataLoader(test_set, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    acc_all = []\n",
    "    f1_all = []\n",
    "    recall_all = []\n",
    "    precision_all = []\n",
    "    for k in range(5):\n",
    "\n",
    "        model = MECM_EDNet(region_list=HEMISPHERE_REGION_LIST, chunk_size=875, num_T=250, num_electrodes=62, hid_channels=32, num_classes_dec=2,num_classes_emo=4).to(device)\n",
    "\n",
    "\n",
    "        \"\"\" Optimizer \"\"\"\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate#, weight_decay=0.0005\n",
    "                                    )\n",
    "\n",
    "    #     train_set = TensorDataset(train_data, train_dec_label, train_emo_label)\n",
    "    #     train_loader = Data.DataLoader(train_set, batch_size=32)\n",
    "\n",
    "    #     test_set = TensorDataset(test_data, test_dec_label, test_emo_label)\n",
    "    #     test_loader = Data.DataLoader(test_set, batch_size=32)\n",
    "\n",
    "        for i in range(epoch_size):                                          #多个epoch\n",
    "\n",
    "            model.train()\n",
    "\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_acc_task = 0.0\n",
    "            positive_iterations = len(positive_dataloader)\n",
    "            negative_iterations = len(negative_dataloader)\n",
    "\n",
    "            # 计算总的迭代次数\n",
    "            total_iterations = min(positive_iterations, negative_iterations)\n",
    "\n",
    "            loop = tqdm(enumerate(zip(positive_dataloader, negative_dataloader)), total=total_iterations)\n",
    "\n",
    "\n",
    "            # 在训练中循环迭代，每次从正样本和负样本数据加载器中分别获取一个批次\n",
    "            for step,((positive_batch,positive_emo), (negative_batch,negative_emo)) in loop:\n",
    "\n",
    "                x=torch.cat((positive_batch,negative_batch),dim=0).to(device)\n",
    "                y1=torch.cat((torch.zeros(len(positive_batch)),torch.ones(len(negative_batch))),dim=0).to(device)\n",
    "                y2=torch.cat((positive_emo,negative_emo),dim=0).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred_task, pred_emo, pred_features = model(x)\n",
    "                loss1 = criterion1(pred_task, y1.long())\n",
    "                loss2 = criterion2(pred_emo,y2.long())\n",
    "                loss3 = criterion3(pred_features,y1)\n",
    "\n",
    "                loss=alpha*loss1+beta*loss2+gamma*loss3\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                pred_task = torch.max(pred_task, 1)[1]\n",
    "                train_correct_task = (pred_task == y1).sum()\n",
    "\n",
    "                train_acc_task += train_correct_task.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loop.set_description(f'Epoch [{i+1} / {epoch_size}]')\n",
    "                loop.set_postfix({\n",
    "                        'loss' : '{:.6f}'.format(train_loss/(length1+length2)),\n",
    "                        'acc_task' : '{:.6f}'.format(train_acc_task*100/(length1+length2))\n",
    "                                                    })\n",
    "                if i+1 == epoch_size and save == True:   \n",
    "                    model_path = './parameter_experiment_new/%s_a=%f_b=%f_g=%f_%d.pkl' % (model_name,alpha,beta,gamma,k+1) #文件夹名称\n",
    "                    os.makedirs('./parameter_experiment_new', exist_ok=True)   #创建文件夹\n",
    "                    state = {'model':model.state_dict()\n",
    "                            }\n",
    "                    torch.save(state,model_path)\n",
    "\n",
    "\n",
    "        prob_all = []\n",
    "        label_all = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()  \n",
    "            for x, y1,y2 in test_loader:\n",
    "                x, y1, y2 =  Variable(x).to(device), Variable(y1).to(device), Variable(y2).to(device)\n",
    "                pred_task, pred_emo,pred_features = model(x)\n",
    "                prob = pred_task.cpu().numpy()\n",
    "                prob_all.extend(np.argmax(prob,axis=1)) #求每一行的最大值索引\n",
    "                label_all.extend(y1.cpu().numpy())\n",
    "            acc_now = accuracy_score(y_true=prob_all, y_pred=label_all)\n",
    "            f1_now = f1_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "            recall_now = recall_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "            precision_now = precision_score(y_true=prob_all, y_pred=label_all, average=\"macro\")\n",
    "            print('accuracy:', np.around(acc_now*100, 2))\n",
    "            print('precision:', np.around(precision_now*100, 2))\n",
    "            print('recall:', np.around(recall_now*100, 2))\n",
    "            print('f1:', np.around(f1_now*100, 2))\n",
    "        acc_all.append(acc_now)\n",
    "        f1_all.append(f1_now)\n",
    "        recall_all.append(recall_now)\n",
    "        precision_all.append(precision_now)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('-'*10, model_name, '-'*10)\n",
    "    print('accuracy:', np.around(np.mean(acc_all)*100, 2), np.std(acc_all))\n",
    "    print('precision:', np.around(np.mean(precision_all)*100, 2), np.std(precision_all))\n",
    "    print('recall:', np.around(np.mean(recall_all)*100, 2), np.std(recall_all))\n",
    "    print('f1:', np.around(np.mean(f1_all)*100, 2), np.std(f1_all))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55297109",
   "metadata": {},
   "outputs": [],
   "source": [
    "MECM_EDNet_learning(1,0.85,0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
